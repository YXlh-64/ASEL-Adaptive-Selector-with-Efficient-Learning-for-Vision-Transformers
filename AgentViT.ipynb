{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a912851e",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Comparative Analysis: AgentViT vs ASEL\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section, we compare two different approaches for efficient Vision Transformer inference:\n",
    "\n",
    "### **Method 1: ASEL (Previous Cells)**\n",
    "- **Approach**: Uses a learned patch selector network that assigns importance scores to patches during training.\n",
    "- **Training**: Selector network trained jointly with ViT using straight-through estimator.\n",
    "- **Inference**: Top-k patches selected based on importance scores, physically removed from computation.\n",
    "- **Key Features**:\n",
    "  - Dynamic budgeting during training (k_ratio varies between 0.15-0.75).\n",
    "  - Sparsity regularization encourages selecting fewer patches.\n",
    "  - Three selection policies for comparison: learned, random, central.\n",
    "\n",
    "### **Method 2: AgentViT (This Section)**\n",
    "- **Approach**: Reinforcement Learning agent (DQN) learns optimal patch selection policy.\n",
    "- **Training**: RL agent trained with reward based on classification accuracy and computational efficiency.\n",
    "- **Inference**: Agent selects patches based on learned Q-values and attention features.\n",
    "- **Key Features**:\n",
    "  - Experience replay buffer for stable RL training.\n",
    "  - Epsilon-greedy exploration strategy.\n",
    "  - Target network for stable Q-learning.\n",
    "  - Global memory mechanism to remember class-specific patch importance patterns.\n",
    "\n",
    "## Comparison Methodology\n",
    "\n",
    "Both methods will be evaluated on the same three datasets:\n",
    "1. **AID** (Aerial Image Dataset)\n",
    "2. **EuroSAT** (Satellite Image Dataset)  \n",
    "3. **RSSCN7** (Remote Sensing Scene Classification)\n",
    "\n",
    "We will compare:\n",
    "- **Accuracy** at different patch retention ratios\n",
    "- **Computational efficiency** (GFLOPs, throughput, latency)\n",
    "- **Patch selection strategies** learned by each method\n",
    "- **Transfer learning capability** from CIFAR-10 to remote sensing domains\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c221700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:23.682524Z",
     "iopub.status.busy": "2025-12-26T13:55:23.682152Z",
     "iopub.status.idle": "2025-12-26T13:55:24.960591Z",
     "shell.execute_reply": "2025-12-26T13:55:24.959979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Common imports and dataset utilities for AgentViT loaded (no ASEL dependency)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMMON IMPORTS AND DATASET UTILITIES (AgentViT Standalone)\n",
    "# ==============================================================================\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Basic configuration shared by AgentViT components (no ASEL dependency)\n",
    "# ------------------------------------------------------------------------------\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'num_workers': 4,\n",
    "    'batch_size': 64,\n",
    "    'resize_dim': 224,\n",
    "    'save_path': './saved_models',\n",
    "    'results_path': './benchmarks_results_agentvit',\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG['save_path'], exist_ok=True)\n",
    "os.makedirs(CONFIG['results_path'], exist_ok=True)\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Dataset loading utilities (mirror Aya_final, but without using ASEL)\n",
    "# ------------------------------------------------------------------------------\n",
    "class CleanImageFolder(datasets.ImageFolder):\n",
    "    def find_classes(self, directory):\n",
    "        \"\"\"Ignore hidden folders and .ipynb_checkpoints.\"\"\"\n",
    "        classes = sorted(\n",
    "            entry.name\n",
    "            for entry in os.scandir(directory)\n",
    "            if entry.is_dir() and not entry.name.startswith('.')\n",
    ")\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"No classes found in {directory}\")\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "# Paths for custom datasets (adjust if your layout differs)\n",
    "DATASET_PATHS = {\n",
    "    'aid': 'AID-data',\n",
    "    'ucmerced': 'UCMerced_LandUse/Images',\n",
    "    'rsscn7': './RSSCN7',\n",
    "}\n",
    "\n",
    "def get_dataset(name: str):\n",
    "    \"\"\"Unified dataset loader for CIFAR-10, EuroSAT, and remote-sensing folders.\"\"\"\n",
    "    tf = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['resize_dim'], CONFIG['resize_dim']), interpolation=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "    ])\n",
    "    \n",
    "    if name == 'cifar10':\n",
    "        ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=tf)\n",
    "        test_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=tf)\n",
    "        return ds, test_ds, 10\n",
    "    elif name == 'eurosat':\n",
    "        ds = datasets.EuroSAT(root='./data', download=True, transform=tf)\n",
    "        train_len = int(0.8 * len(ds))\n",
    "        train_ds, val_ds = random_split(ds, [train_len, len(ds) - train_len])\n",
    "        return train_ds, val_ds, 10\n",
    "    else:\n",
    "        path = DATASET_PATHS.get(name)\n",
    "        if not path or not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Path not found for dataset '{name}': {path}\")\n",
    "        ds = CleanImageFolder(root=path, transform=tf)\n",
    "        train_len = int(0.8 * len(ds))\n",
    "        train_ds, test_ds = random_split(ds, [train_len, len(ds) - train_len])\n",
    "        return train_ds, test_ds, len(ds.classes)\n",
    "\n",
    "print(\"âœ“ Common imports and dataset utilities for AgentViT loaded (no ASEL dependency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23915b",
   "metadata": {},
   "source": [
    "## AgentViT Model Architecture\n",
    "\n",
    "The AgentViT implementation includes:\n",
    "1. **Modified ViT** with dynamic patch masking capability\n",
    "2. **DQN Agent** for learning optimal patch selection\n",
    "3. **Experience Replay** for stable RL training\n",
    "4. **Global Memory** for class-specific patch importance patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f5c16c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:24.962110Z",
     "iopub.status.busy": "2025-12-26T13:55:24.961966Z",
     "iopub.status.idle": "2025-12-26T13:55:25.335999Z",
     "shell.execute_reply": "2025-12-26T13:55:25.335583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AgentViT helper functions and building blocks loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: HELPER FUNCTIONS AND BUILDING BLOCKS\n",
    "# ==============================================================================\n",
    "try:\n",
    "    from einops import rearrange\n",
    "    from einops.layers.torch import Rearrange\n",
    "    HAS_EINOPS = True\n",
    "except ImportError:\n",
    "    HAS_EINOPS = False\n",
    "    print(\"Warning: 'einops' not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'einops'])\n",
    "    from einops import rearrange\n",
    "    from einops.layers.torch import Rearrange\n",
    "    HAS_EINOPS = True\n",
    "\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "except ImportError:\n",
    "    HAS_GYM = False\n",
    "    print(\"Warning: 'gymnasium' not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'gymnasium'])\n",
    "    import gymnasium as gym\n",
    "    HAS_GYM = True\n",
    "\n",
    "from collections import namedtuple\n",
    "import math\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def pair(t):\n",
    "    \"\"\"Convert single value to tuple pair.\"\"\"\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "def posemb_sincos_2d(patches, temperature=10000, dtype=torch.float32):\n",
    "    \"\"\"Generate 2D sinusoidal positional embeddings.\"\"\"\n",
    "    _, h, w, dim, device, dtype = *patches.shape, patches.device, patches.dtype\n",
    "    y, x = torch.meshgrid(torch.arange(h, device=device), torch.arange(w, device=device), indexing='ij')\n",
    "    assert (dim % 4) == 0, 'feature dimension must be multiple of 4 for sincos emb'\n",
    "    omega = torch.arange(dim // 4, device=device) / (dim // 4 - 1)\n",
    "    omega = 1. / (temperature ** omega)\n",
    "    y = y.flatten()[:, None] * omega[None, :]\n",
    "    x = x.flatten()[:, None] * omega[None, :]\n",
    "    pe = torch.cat((x.sin(), x.cos(), y.sin(), y.cos()), dim=1)\n",
    "    return pe.type(dtype)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Feed-Forward Network in Transformer.\"\"\"\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-Head Self-Attention.\"\"\"\n",
    "    def __init__(self, dim, heads=8, dim_head=64):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Stack of Transformer encoder blocks.\"\"\"\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads=heads, dim_head=dim_head),\n",
    "                FeedForward(dim, mlp_dim)\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "print(\"âœ“ AgentViT helper functions and building blocks loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91231c0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.337415Z",
     "iopub.status.busy": "2025-12-26T13:55:25.337296Z",
     "iopub.status.idle": "2025-12-26T13:55:25.341971Z",
     "shell.execute_reply": "2025-12-26T13:55:25.341556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ SimpleAgentViT model loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: CORE MODEL WITH DYNAMIC PATCH SELECTION\n",
    "# ==============================================================================\n",
    "class SimpleAgentViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer with RL-based dynamic patch selection.\n",
    "    Allows selective processing of image patches based on RL agent decisions.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3, dim_head=64):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.selected_patches_mask = []\n",
    "        \n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        \n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n",
    "            'Image dimensions must be divisible by patch size'\n",
    "        \n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        self.num_patches = num_patches\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b h w (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim)\n",
    "        self.to_latent = nn.Identity()\n",
    "        \n",
    "        # Classification head\n",
    "        self.linear_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\"Forward pass with dynamic patch masking.\"\"\"\n",
    "        x = self.to_patch_embedding(img)\n",
    "        pe = posemb_sincos_2d(x)\n",
    "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "        \n",
    "        # Apply patch mask if available\n",
    "        if len(self.selected_patches_mask) > 0:\n",
    "            mask = torch.tensor(self.selected_patches_mask, dtype=torch.bool)\n",
    "            x = x[:, mask, :]\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.to_latent(x)\n",
    "        return self.linear_head(x)\n",
    "    \n",
    "    def set_patches(self, action_q_values):\n",
    "        \"\"\"Convert Q-values to binary patch selection mask using mean threshold.\"\"\"\n",
    "        if isinstance(action_q_values, torch.Tensor):\n",
    "            action_q_values = action_q_values.cpu().detach().numpy()\n",
    "        threshold = np.mean(action_q_values)\n",
    "        self.selected_patches_mask = [1 if val >= threshold else 0 for val in action_q_values]\n",
    "    \n",
    "    def get_patches(self):\n",
    "        \"\"\"Get current patch selection mask.\"\"\"\n",
    "        return self.selected_patches_mask\n",
    "    \n",
    "    def get_att(self, data):\n",
    "        \"\"\"Extract attention features for RL state representation.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.to_patch_embedding(data)\n",
    "            pe = posemb_sincos_2d(x)\n",
    "            x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
    "            \n",
    "            # Get output from first attention layer\n",
    "            if len(self.transformer.layers) > 0:\n",
    "                attn_layer, _ = self.transformer.layers[0]\n",
    "                x_normed = attn_layer.norm(x)\n",
    "                qkv = attn_layer.to_qkv(x_normed).chunk(3, dim=-1)\n",
    "                q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=attn_layer.heads), qkv)\n",
    "                dots = torch.matmul(q, k.transpose(-1, -2)) * attn_layer.scale\n",
    "                attn = attn_layer.attend(dots)\n",
    "                # Average attention across heads and aggregate\n",
    "                attn_features = attn.mean(dim=1).mean(dim=1)  # (batch, num_patches)\n",
    "                return attn_features\n",
    "            else:\n",
    "                return x.mean(dim=-1)\n",
    "\n",
    "print(\"âœ“ SimpleAgentViT model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6179dddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.343329Z",
     "iopub.status.busy": "2025-12-26T13:55:25.343260Z",
     "iopub.status.idle": "2025-12-26T13:55:25.351344Z",
     "shell.execute_reply": "2025-12-26T13:55:25.351094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AgentViT RL components loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: RL ENVIRONMENT AND AGENT\n",
    "# ==============================================================================\n",
    "\n",
    "class ContinuousActionSpace:\n",
    "    \"\"\"Continuous action space for patch selection.\"\"\"\n",
    "    def __init__(self, num_patches, device):\n",
    "        self.num_patches = num_patches\n",
    "        self.device = device\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Sample random action for exploration.\"\"\"\n",
    "        random_action = np.random.rand(self.num_patches)\n",
    "        return torch.tensor(random_action, device=self.device, dtype=torch.float)\n",
    "\n",
    "class ViTEnv(gym.Env):\n",
    "    \"\"\"Custom RL environment for ViT patch selection.\"\"\"\n",
    "    def __init__(self, vit_model, num_patches, optimizer, loss_weight, efficiency_weight,\n",
    "                 device, target_num_patches=1):\n",
    "        super().__init__()\n",
    "        self.vit_model = vit_model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_weight = loss_weight\n",
    "        self.efficiency_weight = efficiency_weight\n",
    "        self.action_space = ContinuousActionSpace(num_patches, device)\n",
    "        self.device = device\n",
    "        self.target_num_patches = target_num_patches\n",
    "        self.train_loss_history = []\n",
    "        self.train_time_history = []\n",
    "    \n",
    "    def step_train(self, action_q_values, train_data, train_target):\n",
    "        \"\"\"Training step without reward computation.\"\"\"\n",
    "        self.vit_model.set_patches(action_q_values)\n",
    "        self.vit_model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = F.log_softmax(self.vit_model(train_data), dim=1)\n",
    "        loss = F.nll_loss(output, train_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def step_reward(self, action_q_values, train_data, train_target):\n",
    "        \"\"\"Training step with reward computation.\"\"\"\n",
    "        self.vit_model.set_patches(action_q_values)\n",
    "        binary_mask = self.vit_model.get_patches()\n",
    "        \n",
    "        # Train and compute loss\n",
    "        start_time = time.time()\n",
    "        self.vit_model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        output = F.log_softmax(self.vit_model(train_data), dim=1)\n",
    "        loss = F.nll_loss(output, train_target)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        iteration_time = time.time() - start_time\n",
    "        \n",
    "        self.train_loss_history.append(loss.item())\n",
    "        self.train_time_history.append(iteration_time)\n",
    "        \n",
    "        # Compute reward\n",
    "        num_selected = binary_mask.count(1)\n",
    "        efficiency_reward = -abs(num_selected - self.target_num_patches) / self.target_num_patches\n",
    "        loss_improvement = self.train_loss_history[0] / (loss.item() + 1e-8)\n",
    "        reward = loss_improvement * self.loss_weight + efficiency_reward * self.efficiency_weight\n",
    "        \n",
    "        next_state = self.get_state(train_data)\n",
    "        return next_state, reward\n",
    "    \n",
    "    def get_state(self, data):\n",
    "        \"\"\"Get state representation (attention features).\"\"\"\n",
    "        return self.vit_model.get_att(data)\n",
    "\n",
    "Experience = namedtuple('Experience', ('state', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for DQN.\"\"\"\n",
    "    def __init__(self, capacity, sample_batch_size):\n",
    "        self.sample_batch_size = sample_batch_size\n",
    "        self.memory = []\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, *args):\n",
    "        if len(self.memory) >= self.capacity:\n",
    "            self.memory.pop(random.randint(0, len(self.memory) - 1))\n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.sample_batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Q-Network for predicting patch importance.\"\"\"\n",
    "    def __init__(self, num_patches):\n",
    "        super().__init__()\n",
    "        input_dim = num_patches * 2  # state + memory\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_patches)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.fc_layers(state)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent for learning patch selection.\"\"\"\n",
    "    def __init__(self, replay_batch_size, num_patches, buffer_capacity, gamma, tau,\n",
    "                 update_frequency, learning_rate, env, device, num_classes=None):\n",
    "        self.replay_batch_size = replay_batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.steps_since_update = 0\n",
    "        self.update_frequency = update_frequency\n",
    "        self.device = device\n",
    "        self.env = env\n",
    "        self.num_patches = num_patches\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Global memory for class-specific patterns\n",
    "        if num_classes:\n",
    "            self.global_memory = torch.zeros(num_classes, num_patches, device=device, dtype=torch.float32)\n",
    "        else:\n",
    "            self.global_memory = None\n",
    "        self.memory_alpha = 0.01\n",
    "        \n",
    "        # Q-networks\n",
    "        self.q_network = QNetwork(num_patches).to(device)\n",
    "        self.target_network = QNetwork(num_patches).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), lr=learning_rate, amsgrad=True)\n",
    "        self.memory = ReplayBuffer(buffer_capacity, replay_batch_size)\n",
    "    \n",
    "    def augment_state(self, state, labels=None):\n",
    "        \"\"\"Augment state with class-specific memory.\"\"\"\n",
    "        state = state.to(self.device)\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        if self.global_memory is None or self.num_classes is None:\n",
    "            mem = torch.zeros((batch_size, self.num_patches), device=self.device, dtype=state.dtype)\n",
    "        else:\n",
    "            if labels is None:\n",
    "                mean_mem = torch.mean(self.global_memory, dim=0, keepdim=True)\n",
    "                mem = mean_mem.repeat(batch_size, 1)\n",
    "            else:\n",
    "                labels_cpu = labels.detach().cpu().long().numpy()\n",
    "                mem_list = [self.global_memory[int(c)].unsqueeze(0) for c in labels_cpu]\n",
    "                mem = torch.cat(mem_list, dim=0).to(self.device)\n",
    "        \n",
    "        return torch.cat([state, mem], dim=1)\n",
    "    \n",
    "    def update_global_memory(self, labels, binary_mask):\n",
    "        \"\"\"Update class-specific memory with EMA.\"\"\"\n",
    "        if self.global_memory is None:\n",
    "            return\n",
    "        if not isinstance(binary_mask, torch.Tensor):\n",
    "            mask = torch.tensor(binary_mask, dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            mask = binary_mask.to(self.device).float()\n",
    "        \n",
    "        labels_cpu = labels.detach().cpu().long().numpy()\n",
    "        for c in labels_cpu:\n",
    "            c = int(c)\n",
    "            self.global_memory[c] = (1.0 - self.memory_alpha) * self.global_memory[c] + self.memory_alpha * mask\n",
    "    \n",
    "    def select_action(self, state, labels=None, epsilon=0.):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                augmented = self.augment_state(state, labels)\n",
    "                batch_q_values = self.q_network(augmented)\n",
    "                action_q_values = torch.mean(batch_q_values, dim=0)\n",
    "                return action_q_values\n",
    "        else:\n",
    "            return self.env.action_space.sample()\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        \"\"\"DQN optimization step.\"\"\"\n",
    "        if len(self.memory) < self.replay_batch_size:\n",
    "            return\n",
    "        \n",
    "        experiences = self.memory.sample()\n",
    "        batch = Experience(*zip(*experiences))\n",
    "        \n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        next_state_batch = torch.cat(batch.next_state).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        \n",
    "        current_q = self.q_network(state_batch)\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state_batch)\n",
    "        \n",
    "        target_q = (next_q * self.gamma) + reward_batch.unsqueeze(1)\n",
    "        loss = F.smooth_l1_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.q_network.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps_since_update += 1\n",
    "        if self.steps_since_update >= self.update_frequency:\n",
    "            self._soft_update()\n",
    "            self.steps_since_update = 0\n",
    "    \n",
    "    def _soft_update(self):\n",
    "        \"\"\"Soft update of target network.\"\"\"\n",
    "        target_dict = self.target_network.state_dict()\n",
    "        online_dict = self.q_network.state_dict()\n",
    "        for key in online_dict:\n",
    "            target_dict[key] = online_dict[key] * self.tau + target_dict[key] * (1 - self.tau)\n",
    "        self.target_network.load_state_dict(target_dict)\n",
    "\n",
    "print(\"âœ“ AgentViT RL components loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4391b8a",
   "metadata": {},
   "source": [
    "## AgentViT Training and Evaluation Functions\n",
    "\n",
    "Training orchestrator that jointly trains the ViT and RL agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb8a3e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.352398Z",
     "iopub.status.busy": "2025-12-26T13:55:25.352328Z",
     "iopub.status.idle": "2025-12-26T13:55:25.356671Z",
     "shell.execute_reply": "2025-12-26T13:55:25.356442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AgentViT trainer loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: TRAINING ORCHESTRATOR\n",
    "# ==============================================================================\n",
    "class AgentViTTrainer:\n",
    "    \"\"\"Orchestrates joint training of ViT and DQN agent.\"\"\"\n",
    "    def __init__(self, vit_model, num_patches, num_epochs, env, train_loader, test_loader, device,\n",
    "                 dqn_config):\n",
    "        self.env = env\n",
    "        self.num_epochs = num_epochs\n",
    "        self.vit_model = vit_model\n",
    "        self.num_patches = num_patches\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Extract DQN config\n",
    "        self.epsilon_start = dqn_config.get('epsilon_start', 1.0)\n",
    "        self.epsilon_end = dqn_config.get('epsilon_end', 0.01)\n",
    "        self.epsilon_decay = dqn_config.get('epsilon_decay', 20000)\n",
    "        self.reward_frequency = dqn_config.get('reward_frequency', 50)\n",
    "        \n",
    "        # Create DQN agent\n",
    "        self.dqn_agent = DQNAgent(\n",
    "            replay_batch_size=dqn_config.get('replay_batch_size', 8),\n",
    "            num_patches=num_patches,\n",
    "            buffer_capacity=dqn_config.get('buffer_capacity', 64),\n",
    "            gamma=dqn_config.get('gamma', 0.95),\n",
    "            tau=dqn_config.get('tau', 0.1),\n",
    "            update_frequency=dqn_config.get('update_frequency', 2),\n",
    "            learning_rate=dqn_config.get('learning_rate', 0.01),\n",
    "            env=self.env,\n",
    "            device=device,\n",
    "            num_classes=vit_model.num_classes\n",
    "        )\n",
    "        \n",
    "        self.val_acc_history = []\n",
    "        self.val_loss_history = []\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        total_iterations = 0\n",
    "        epsilon = self.epsilon_start\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f'\\n{\"=\"*60}\\nEpoch {epoch+1}/{self.num_epochs}\\n{\"=\"*60}')\n",
    "            \n",
    "            for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                total_iterations += 1\n",
    "                \n",
    "                # Get state and select action\n",
    "                state = self.env.get_state(images)\n",
    "                action = self.dqn_agent.select_action(state, labels, epsilon)\n",
    "                \n",
    "                # Training step\n",
    "                if batch_idx % self.reward_frequency != 0:\n",
    "                    self.env.step_train(action, images, labels)\n",
    "                else:\n",
    "                    next_state, reward = self.env.step_reward(action, images, labels)\n",
    "                    reward_tensor = torch.full((images.size(0),), reward, dtype=torch.float32)\n",
    "                    \n",
    "                    if epoch > 0:  # Skip first epoch\n",
    "                        binary_mask = self.vit_model.get_patches()\n",
    "                        if binary_mask:\n",
    "                            self.dqn_agent.update_global_memory(labels, binary_mask)\n",
    "                        \n",
    "                        aug_state = self.dqn_agent.augment_state(state, labels)\n",
    "                        aug_next = self.dqn_agent.augment_state(next_state, labels)\n",
    "                        self.dqn_agent.memory.push(aug_state, aug_next, reward_tensor)\n",
    "                        self.dqn_agent.optimize_model()\n",
    "                \n",
    "                # Decay epsilon\n",
    "                epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \\\n",
    "                          math.exp(-1. * total_iterations / self.epsilon_decay)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.evaluate()\n",
    "            self.val_acc_history.append(val_acc)\n",
    "            self.val_loss_history.append(val_loss)\n",
    "            print(f'Validation - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%')\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate on test set with all patches.\"\"\"\n",
    "        original_mask = self.vit_model.get_patches()\n",
    "        self.vit_model.set_patches(torch.ones(self.num_patches))\n",
    "        \n",
    "        self.vit_model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in self.test_loader:\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                output = F.log_softmax(self.vit_model(images), dim=1)\n",
    "                loss = F.nll_loss(output, labels, reduction='sum')\n",
    "                total_loss += loss.item()\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        if original_mask:\n",
    "            self.vit_model.set_patches(torch.tensor(original_mask, dtype=torch.float))\n",
    "        \n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "print(\"âœ“ AgentViT trainer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dfe1d5",
   "metadata": {},
   "source": [
    "## AgentViT Benchmarking Functions\n",
    "\n",
    "Functions to evaluate AgentViT performance at different patch retention ratios, matching the ASEL evaluation methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c0747c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.358013Z",
     "iopub.status.busy": "2025-12-26T13:55:25.357944Z",
     "iopub.status.idle": "2025-12-26T13:55:25.365047Z",
     "shell.execute_reply": "2025-12-26T13:55:25.364842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AgentViT benchmarking functions loaded\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: BENCHMARKING UTILITIES\n",
    "# ==============================================================================\n",
    "class AgentViTBenchmark:\n",
    "    \"\"\"Benchmarking utilities for AgentViT evaluation.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def set_patch_ratio(model, k_ratio):\n",
    "        \"\"\"Set a specific number of patches to be selected.\"\"\"\n",
    "        num_patches = model.num_patches\n",
    "        k = max(1, int(num_patches * k_ratio))\n",
    "        # Set top-k patches to 1, rest to 0\n",
    "        mask = [1 if i < k else 0 for i in range(num_patches)]\n",
    "        model.selected_patches_mask = mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_accuracy_at_ratio(model, loader, device, k_ratio, use_learned_selection=False, dqn_agent=None):\n",
    "        \"\"\"\n",
    "        Evaluate accuracy at a specific patch retention ratio.\n",
    "        \n",
    "        Args:\n",
    "            model: AgentViT model\n",
    "            loader: Data loader\n",
    "            device: Device\n",
    "            k_ratio: Patch retention ratio (0.0 to 1.0)\n",
    "            use_learned_selection: If True and dqn_agent provided, use agent's selection\n",
    "            dqn_agent: Optional DQN agent for learned selection\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if use_learned_selection and dqn_agent is not None:\n",
    "                    # Use learned patch selection from agent\n",
    "                    state = model.get_att(images)\n",
    "                    action = dqn_agent.select_action(state, labels, epsilon=0.0)\n",
    "                    model.set_patches(action)\n",
    "                    \n",
    "                    # Adjust to meet k_ratio constraint\n",
    "                    current_mask = model.get_patches()\n",
    "                    num_selected = sum(current_mask)\n",
    "                    target_k = max(1, int(model.num_patches * k_ratio))\n",
    "                    \n",
    "                    if num_selected != target_k:\n",
    "                        # Adjust mask to match target ratio\n",
    "                        if isinstance(action, torch.Tensor):\n",
    "                            action_np = action.cpu().numpy()\n",
    "                        else:\n",
    "                            action_np = action\n",
    "                        top_k_indices = np.argsort(action_np)[-target_k:]\n",
    "                        new_mask = [1 if i in top_k_indices else 0 for i in range(model.num_patches)]\n",
    "                        model.selected_patches_mask = new_mask\n",
    "                else:\n",
    "                    # Use simple top-k selection\n",
    "                    AgentViTBenchmark.set_patch_ratio(model, k_ratio)\n",
    "                \n",
    "                # Forward pass\n",
    "                output = model(images)\n",
    "                pred = output.argmax(dim=1)\n",
    "                correct += pred.eq(labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def measure_metrics(model, device, k_ratio):\n",
    "        \"\"\"\n",
    "        Measure computational metrics (GFLOPs, latency, throughput).\n",
    "        Simplified version matching Benchmark class from Prunable ViT.\n",
    "        \"\"\"\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        batch_input = torch.randn(64, 3, 224, 224).to(device)\n",
    "        \n",
    "        # Set patch ratio\n",
    "        AgentViTBenchmark.set_patch_ratio(model, k_ratio)\n",
    "        \n",
    "        # Estimate GFLOPs (theoretical approximation)\n",
    "        gflops = 1.1 * k_ratio  # Simplified estimate\n",
    "        \n",
    "        # Measure latency and throughput\n",
    "        model.eval()\n",
    "        start_event = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        end_event = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(batch_input)  # Warmup\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "                for _ in range(50):\n",
    "                    _ = model(batch_input)\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                total_time_ms = start_event.elapsed_time(end_event)\n",
    "            else:\n",
    "                start = time.time()\n",
    "                for _ in range(50):\n",
    "                    _ = model(batch_input)\n",
    "                total_time_ms = (time.time() - start) * 1000\n",
    "            \n",
    "            latency_ms = total_time_ms / 50\n",
    "            throughput = (64 * 50) / (total_time_ms / 1000)\n",
    "        \n",
    "        return gflops, latency_ms, throughput\n",
    "\n",
    "def run_agentvit_benchmarks(model, test_loader, ds_name, device, dqn_agent=None):\n",
    "    \"\"\"\n",
    "    Run comprehensive benchmarks for AgentViT, matching Prunable ViT evaluation.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained AgentViT model\n",
    "        test_loader: Test data loader\n",
    "        ds_name: Dataset name for plot labels\n",
    "        device: Device\n",
    "        dqn_agent: Optional trained DQN agent for learned selection\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\\nRunning AgentViT Benchmarks for {ds_name}...\\n{'='*60}\")\n",
    "    \n",
    "    ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    res = {\n",
    "        'ratios': ratios,\n",
    "        'acc_learned': [],\n",
    "        'acc_random': [],\n",
    "        'gflops': [],\n",
    "        'thr': [],\n",
    "        'lat': []\n",
    "    }\n",
    "    \n",
    "    # Baseline (100%)\n",
    "    full_acc = AgentViTBenchmark.evaluate_accuracy_at_ratio(\n",
    "        model, test_loader, device, 1.0, use_learned_selection=False\n",
    "    )\n",
    "    full_gflops, full_lat, full_thr = AgentViTBenchmark.measure_metrics(model, device, 1.0)\n",
    "    \n",
    "    for r in ratios:\n",
    "        print(f\"Evaluating at ratio {r:.1f}...\")\n",
    "        \n",
    "        # Learned selection (using DQN agent if available)\n",
    "        acc_learned = AgentViTBenchmark.evaluate_accuracy_at_ratio(\n",
    "            model, test_loader, device, r, use_learned_selection=(dqn_agent is not None), dqn_agent=dqn_agent\n",
    "        )\n",
    "        \n",
    "        # Random baseline (simple top-k without learning)\n",
    "        acc_random = AgentViTBenchmark.evaluate_accuracy_at_ratio(\n",
    "            model, test_loader, device, r, use_learned_selection=False\n",
    "        )\n",
    "        \n",
    "        # Metrics\n",
    "        gf, lat, thr = AgentViTBenchmark.measure_metrics(model, device, r)\n",
    "        \n",
    "        res['acc_learned'].append(acc_learned)\n",
    "        res['acc_random'].append(acc_random)\n",
    "        res['gflops'].append(gf)\n",
    "        res['thr'].append(thr)\n",
    "        res['lat'].append(lat)\n",
    "        \n",
    "        print(f\"  Ratio {r:.1f} | Learned-Acc: {acc_learned:.1%} | Random-Acc: {acc_random:.1%} | FPS: {thr:.0f}\")\n",
    "    \n",
    "    # --- PLOTTING (matching Prunable ViT style) ---\n",
    "    results_path = CONFIG['results_path']\n",
    "    \n",
    "    # 1. Strategy Comparison\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ratios, [x*100 for x in res['acc_learned']], 'r-o', lw=2, label='AgentViT (Learned)')\n",
    "    plt.plot(ratios, [x*100 for x in res['acc_random']], 'k--x', alpha=0.5, label='Random')\n",
    "    plt.scatter([1.0], [full_acc*100], c='k', marker='*', s=200, zorder=10, label='Full ViT')\n",
    "    plt.title(f'{ds_name}: AgentViT Strategy Comparison')\n",
    "    plt.xlabel('Keep Ratio')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{results_path}/{ds_name}_agentvit_1_strategies.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Acc vs Throughput\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(res['thr'], [x*100 for x in res['acc_learned']], 'g-o', lw=2, label='AgentViT')\n",
    "    plt.scatter([full_thr], [full_acc*100], c='k', marker='*', s=200, label='Full ViT')\n",
    "    plt.title(f'{ds_name}: AgentViT Accuracy vs Throughput')\n",
    "    plt.xlabel('Throughput (img/s)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{results_path}/{ds_name}_agentvit_2_throughput.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. GFLOPs\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ratios, res['gflops'], 'm-o', lw=2, label='AgentViT')\n",
    "    plt.axhline(y=full_gflops, c='k', ls='--', label='Full ViT')\n",
    "    plt.title(f'{ds_name}: AgentViT GFLOPs Reduction')\n",
    "    plt.xlabel('Keep Ratio')\n",
    "    plt.ylabel('GFLOPs')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{results_path}/{ds_name}_agentvit_3_gflops.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Latency Bar\n",
    "    lat_50 = res['lat'][4]  # Ratio 0.5\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.bar(['Full ViT', 'AgentViT (50%)'], [full_lat, lat_50], color=['gray', 'green'], width=0.5)\n",
    "    plt.title(f'{ds_name}: AgentViT Batch Latency')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.text(0, full_lat, f\"{full_lat:.1f}ms\", ha='center', va='bottom', fontweight='bold')\n",
    "    plt.text(1, lat_50, f\"{lat_50:.1f}ms\", ha='center', va='bottom', fontweight='bold')\n",
    "    plt.savefig(f\"{results_path}/{ds_name}_agentvit_4_latency.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Finished AgentViT benchmarks for {ds_name}. Plots saved.\\n\")\n",
    "    \n",
    "    return res\n",
    "\n",
    "print(\"âœ“ AgentViT benchmarking functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fd844",
   "metadata": {},
   "source": [
    "## AgentViT Experimental Pipeline\n",
    "\n",
    "Testing AgentViT on the same three datasets used for ASEL evaluation (AID, EuroSAT, RSSCN7),\n",
    "but **training AgentViT independently on each dataset** (no CIFAR-10 transfer learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f825c78e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.366312Z",
     "iopub.status.busy": "2025-12-26T13:55:25.366244Z",
     "iopub.status.idle": "2025-12-26T13:55:25.370471Z",
     "shell.execute_reply": "2025-12-26T13:55:25.370223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AgentViT experimental pipeline ready (no transfer learning)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# AGENTVIT: MAIN EXPERIMENTAL PIPELINE\n",
    "# ==============================================================================\n",
    "def run_agentvit_experiments():\n",
    "    \"\"\"\n",
    "    Main pipeline for AgentViT experiments.\n",
    "\n",
    "    Trains and evaluates AgentViT **separately on each dataset** (AID, EuroSAT, RSSCN7),\n",
    "    without any CIFAR-10 warmup or transfer learning, so that AgentViT keeps its\n",
    "    original training scheme while still being evaluated on the same datasets as ASEL.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'#'*80}\\n# AGENTVIT EXPERIMENTAL PIPELINE\\n{'#'*80}\\n\")\n",
    "\n",
    "    # Configuration for AgentViT (keeps original structure, no transfer learning)\n",
    "    AGENTVIT_CONFIG = {\n",
    "        'seed': 42,\n",
    "        'device': CONFIG['device'],\n",
    "        'batch_size': 32,  # Smaller batch for RL stability\n",
    "        'num_workers': 4,\n",
    "        'resize_dim': 224,\n",
    "\n",
    "        # ViT architecture (matching image size)\n",
    "        'patch_size': 16,  # 224/16 = 14x14 = 196 patches\n",
    "        'embed_dim': 128,\n",
    "        'depth': 6,\n",
    "        'heads': 8,\n",
    "        'mlp_dim': 512,\n",
    "        'vit_lr': 1e-3,\n",
    "\n",
    "        # RL/DQN parameters\n",
    "        'epsilon_start': 1.0,\n",
    "        'epsilon_end': 0.01,\n",
    "        'epsilon_decay': 20000,\n",
    "        'replay_batch_size': 8,\n",
    "        'buffer_capacity': 64,\n",
    "        'gamma': 0.95,\n",
    "        'tau': 0.1,\n",
    "        'update_frequency': 2,\n",
    "        'dqn_lr': 0.01,\n",
    "        'reward_frequency': 50,\n",
    "\n",
    "        # Reward function\n",
    "        'target_patches': 98,  # ~50% of 196 patches\n",
    "        'efficiency_weight': 20.0,\n",
    "        'loss_weight': 1.0,\n",
    "\n",
    "        # Training epochs per dataset\n",
    "        'num_epochs': 20,\n",
    "    }\n",
    "\n",
    "    set_seed(AGENTVIT_CONFIG['seed'])\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # TRAIN AND EVALUATE ON EACH TARGET DATASET (NO TRANSFER)\n",
    "    # ------------------------------------------------------------------\n",
    "    target_datasets = ['aid', 'eurosat', 'rsscn7']\n",
    "\n",
    "    # Number of patches is fixed by image/patch size\n",
    "    num_patches_per_side = AGENTVIT_CONFIG['resize_dim'] // AGENTVIT_CONFIG['patch_size']\n",
    "    num_patches = num_patches_per_side * num_patches_per_side\n",
    "\n",
    "    for ds_name in target_datasets:\n",
    "        print(f\"\\n{'='*60}\\nTRAINING AGENTVIT ON {ds_name.upper()} (from scratch)\\n{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            # Load dataset\n",
    "            train_ds, test_ds, n_cls = get_dataset(ds_name)\n",
    "            train_loader = DataLoader(\n",
    "                train_ds,\n",
    "                batch_size=AGENTVIT_CONFIG['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=AGENTVIT_CONFIG['num_workers'],\n",
    "            )\n",
    "            test_loader = DataLoader(\n",
    "                test_ds,\n",
    "                batch_size=AGENTVIT_CONFIG['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=AGENTVIT_CONFIG['num_workers'],\n",
    "                drop_last=True,\n",
    "            )\n",
    "\n",
    "            # Create AgentViT model for this dataset\n",
    "            agentvit_model = SimpleAgentViT(\n",
    "                image_size=AGENTVIT_CONFIG['resize_dim'],\n",
    "                patch_size=AGENTVIT_CONFIG['patch_size'],\n",
    "                num_classes=n_cls,\n",
    "                dim=AGENTVIT_CONFIG['embed_dim'],\n",
    "                depth=AGENTVIT_CONFIG['depth'],\n",
    "                heads=AGENTVIT_CONFIG['heads'],\n",
    "                mlp_dim=AGENTVIT_CONFIG['mlp_dim'],\n",
    "            ).to(AGENTVIT_CONFIG['device'])\n",
    "\n",
    "            # Optimizer for ViT\n",
    "            vit_optimizer = optim.Adam(agentvit_model.parameters(), lr=AGENTVIT_CONFIG['vit_lr'])\n",
    "\n",
    "            # Create RL environment\n",
    "            env = ViTEnv(\n",
    "                vit_model=agentvit_model,\n",
    "                num_patches=num_patches,\n",
    "                optimizer=vit_optimizer,\n",
    "                loss_weight=AGENTVIT_CONFIG['loss_weight'],\n",
    "                efficiency_weight=AGENTVIT_CONFIG['efficiency_weight'],\n",
    "                device=AGENTVIT_CONFIG['device'],\n",
    "                target_num_patches=AGENTVIT_CONFIG['target_patches'],\n",
    "            )\n",
    "\n",
    "            # DQN config\n",
    "            dqn_config = {\n",
    "                'epsilon_start': AGENTVIT_CONFIG['epsilon_start'],\n",
    "                'epsilon_end': AGENTVIT_CONFIG['epsilon_end'],\n",
    "                'epsilon_decay': AGENTVIT_CONFIG['epsilon_decay'],\n",
    "                'replay_batch_size': AGENTVIT_CONFIG['replay_batch_size'],\n",
    "                'buffer_capacity': AGENTVIT_CONFIG['buffer_capacity'],\n",
    "                'gamma': AGENTVIT_CONFIG['gamma'],\n",
    "                'tau': AGENTVIT_CONFIG['tau'],\n",
    "                'update_frequency': AGENTVIT_CONFIG['update_frequency'],\n",
    "                'learning_rate': AGENTVIT_CONFIG['dqn_lr'],\n",
    "                'reward_frequency': AGENTVIT_CONFIG['reward_frequency'],\n",
    "            }\n",
    "\n",
    "            # Create trainer (keeps original AgentViT training logic)\n",
    "            trainer = AgentViTTrainer(\n",
    "                vit_model=agentvit_model,\n",
    "                num_patches=num_patches,\n",
    "                num_epochs=AGENTVIT_CONFIG['num_epochs'],\n",
    "                env=env,\n",
    "                train_loader=train_loader,\n",
    "                test_loader=test_loader,\n",
    "                device=AGENTVIT_CONFIG['device'],\n",
    "                dqn_config=dqn_config,\n",
    "            )\n",
    "\n",
    "            # Train on this dataset\n",
    "            print(f\"\\nTraining AgentViT on {ds_name}...\")\n",
    "            trainer.train()\n",
    "\n",
    "            # Benchmark on this dataset\n",
    "            print(f\"\\nBenchmarking AgentViT on {ds_name}...\")\n",
    "            run_agentvit_benchmarks(\n",
    "                agentvit_model,\n",
    "                test_loader,\n",
    "                ds_name,\n",
    "                AGENTVIT_CONFIG['device'],\n",
    "                dqn_agent=trainer.dqn_agent,\n",
    "            )\n",
    "\n",
    "            # Save trained model for this dataset\n",
    "            save_path = f\"{CONFIG['save_path']}/agentvit_{ds_name}.pth\"\n",
    "            torch.save({\n",
    "                'model_state': agentvit_model.state_dict(),\n",
    "                'dqn_state': trainer.dqn_agent.q_network.state_dict(),\n",
    "                'config': AGENTVIT_CONFIG,\n",
    "            }, save_path)\n",
    "            print(f\"âœ“ Saved AgentViT model for {ds_name} to {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ds_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n{'#'*80}\\n# AGENTVIT EXPERIMENTS COMPLETED\\n{'#'*80}\\n\")\n",
    "\n",
    "print(\"âœ“ AgentViT experimental pipeline ready (no transfer learning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f31e6",
   "metadata": {},
   "source": [
    "## Execute AgentViT Experiments\n",
    "\n",
    "Run this cell to execute the complete AgentViT experimental pipeline on all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d84787",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.371528Z",
     "iopub.status.busy": "2025-12-26T13:55:25.371462Z",
     "iopub.status.idle": "2025-12-26T13:55:25.373113Z",
     "shell.execute_reply": "2025-12-26T13:55:25.372857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘                   AGENTVIT EXPERIMENTS READY TO RUN                    â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "To execute the AgentViT experiments:\n",
      "1. Uncomment the line: run_agentvit_experiments()\n",
      "2. Run this cell\n",
      "\n",
      "The pipeline will:\n",
      "- Train AgentViT **from scratch** on each of: AID, EuroSAT, and RSSCN7\n",
      "- Use the original RL-based AgentViT training logic (no CIFAR-10 transfer)\n",
      "- Generate benchmark plots for each dataset (accuracy vs keep ratio, throughput, GFLOPs, latency)\n",
      "- Save all results to the configured paths\n",
      "\n",
      "Expected runtime: several hours in total (depending on GPU)\n",
      "\n",
      "Results will be saved to: ./benchmarks_results_agentvit\n",
      "Models will be saved to: ./saved_models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# RUN AGENTVIT EXPERIMENTS\n",
    "# ==============================================================================\n",
    "# Uncomment the line below to run the AgentViT experiments\n",
    "# Note: This will take several hours depending on your hardware,\n",
    "#!/\n",
    "#! Recommended to start with one dataset (e.g., only AID) by\n",
    "#! temporarily editing target_datasets inside run_agentvit_experiments().\n",
    "\n",
    "# run_agentvit_experiments()\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                   AGENTVIT EXPERIMENTS READY TO RUN                    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "To execute the AgentViT experiments:\n",
    "1. Uncomment the line: run_agentvit_experiments()\n",
    "2. Run this cell\n",
    "\n",
    "The pipeline will:\n",
    "- Train AgentViT **from scratch** on each of: AID, EuroSAT, and RSSCN7\n",
    "- Use the original RL-based AgentViT training logic (no CIFAR-10 transfer)\n",
    "- Generate benchmark plots for each dataset (accuracy vs keep ratio, throughput, GFLOPs, latency)\n",
    "- Save all results to the configured paths\n",
    "\n",
    "Expected runtime: several hours in total (depending on GPU)\n",
    "\n",
    "Results will be saved to: {0}\n",
    "Models will be saved to: {1}\n",
    "\"\"\".format(CONFIG['results_path'], CONFIG['save_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9966e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.373912Z",
     "iopub.status.busy": "2025-12-26T13:55:25.373843Z",
     "iopub.status.idle": "2025-12-26T13:55:25.375230Z",
     "shell.execute_reply": "2025-12-26T13:55:25.374996Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentViT training is disabled. Notebook will only use existing checkpoints in 'saved_models' and run comparisons.\n"
     ]
    }
   ],
   "source": [
    "# Run the AgentViT experimental pipeline (DISABLED to avoid retraining)\n",
    "# If you ever want to retrain AgentViT models, uncomment the lines below\n",
    "# and run this cell manually.\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_agentvit_experiments()\n",
    "\n",
    "print(\"AgentViT training is disabled. Notebook will only use existing checkpoints in 'saved_models' and run comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49269c99",
   "metadata": {},
   "source": [
    "## Conceptual Summary: ASEL vs AgentViT\n",
    "\n",
    "### ASEL (Original Pipeline)\n",
    "- Learns a continuous importance score for each patch using a small selector network.\n",
    "- During training, a straight-through estimator is used to keep top-k patches while still allowing gradients to flow.\n",
    "- At inference, the model physically prunes away low-importance patches (learned, random, central policies).\n",
    "- Uses a **CIFAR-10 warmup + transfer learning** strategy: first train on CIFAR-10, then fine-tune on AID, EuroSAT, and RSSCN7.\n",
    "- Evaluation measures how accuracy changes as we reduce the number of patches, along with GFLOPs, throughput, and latency.\n",
    "\n",
    "### AgentViT (Added Pipeline)\n",
    "- Treats patch selection as a reinforcement learning problem.\n",
    "- A DQN agent observes attention-based features from the ViT and outputs Q-values for each patch.\n",
    "- Q-values are converted into binary masks (selected/discarded) and used to restrict computation to selected patches.\n",
    "- The reward balances two components:\n",
    "  - **Classification quality**: how much the loss improves\n",
    "  - **Efficiency**: how close the number of selected patches is to a target budget\n",
    "- The agent is trained with experience replay, target networks, and epsilon-greedy exploration.\n",
    "- In this baseline notebook, **AgentViT is trained independently on each dataset (AID, EuroSAT, RSSCN7)**, without CIFAR-10 warmup or transfer learning.\n",
    "\n",
    "### Comparison Protocol\n",
    "- **ASEL**: CIFAR-10 warmup, then transfer and fine-tune on AID, EuroSAT, and RSSCN7.\n",
    "- **AgentViT**: Direct training on each target dataset from scratch using its RL-based patch selection.\n",
    "- **Metrics**:\n",
    "  - Accuracy at multiple keep ratios (10%â€“100% of patches)\n",
    "  - GFLOPs reduction as patches are pruned\n",
    "  - Throughput (images per second) and latency (ms) for a fixed batch size\n",
    "- **Interpretation**:\n",
    "  - If AgentViT maintains higher accuracy at lower keep ratios, it indicates a more effective patch selection policy on that dataset, even without transfer learning.\n",
    "  - If ASEL achieves similar accuracy with simpler training and transfer, it may be more practical despite lacking RL flexibility.\n",
    "\n",
    "This setup allows a controlled, side-by-side comparison between a **deterministic learned selector** (ASEL, with transfer learning) and an **RL-based adaptive selector** (AgentViT, trained per dataset) under the same datasets, image resolution, and evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5b619",
   "metadata": {},
   "source": [
    "## Comparative Plots from Saved Models\n",
    "\n",
    "This section loads the **saved ASEL checkpoints** (`aid_finetuned.pth`, `eurosat_finetuned.pth`, `rsscn7_finetuned.pth`)\n",
    "and the **saved AgentViT checkpoints** (`agentvit_aid.pth`, `agentvit_eurosat.pth`, `agentvit_rsscn7.pth`) from the `saved_models`\n",
    "directory, then computes and plots **comparative curves** for ASEL vs AgentViT on each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa2d6dea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.375963Z",
     "iopub.status.busy": "2025-12-26T13:55:25.375896Z",
     "iopub.status.idle": "2025-12-26T13:55:25.523840Z",
     "shell.execute_reply": "2025-12-26T13:55:25.523339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ ASEL model and benchmark helpers loaded (for comparison)\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ASEL MODEL DEFINITION AND BENCHMARK HELPERS (FROM ASEL2, INFERENCE ONLY)\n",
    "# ==============================================================================\n",
    "import timm\n",
    "\n",
    "try:\n",
    "    from fvcore.nn import FlopCountAnalysis\n",
    "    HAS_FVCORE = True\n",
    "except ImportError:\n",
    "    HAS_FVCORE = False\n",
    "    print(\"Warning: 'fvcore' not found. ASEL GFLOPs will be estimated theoretically.\")\n",
    "\n",
    "class ASEL(nn.Module):\n",
    "    \"\"\"\n",
    "    ASEL model (same as PrunableViT in ASEL2) used here for\n",
    "    loading saved checkpoints and running benchmarks (no training).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_tiny_patch16_224', pretrained=pretrained, num_classes=num_classes)\n",
    "        self.embed_dim = self.backbone.embed_dim\n",
    "        self.patch_selector = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim * 2, 96),\n",
    "            nn.LayerNorm(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(96, 1),\n",
    "            nn.Sigmoid(),\n",
    ")\n",
    "        H, W = 14, 14\n",
    "        center = (H - 1) / 2.0\n",
    "        y, x = np.ogrid[:H, :W]\n",
    "        dist = (x - center) ** 2 + (y - center) ** 2\n",
    "        self.central_indices = torch.from_numpy(np.argsort(dist.flatten())).long()\n",
    "\n",
    "    def _get_patch_embeddings(self, x):\n",
    "        x = self.backbone.patch_embed(x)\n",
    "        x = x + self.backbone.pos_embed[:, 1:]\n",
    "        return x\n",
    "\n",
    "    def _process_transformer(self, x_patches):\n",
    "        B = x_patches.shape[0]\n",
    "        cls_token = self.backbone.cls_token.expand(B, -1, -1) + self.backbone.pos_embed[:, :1]\n",
    "        x = torch.cat((cls_token, x_patches), dim=1)\n",
    "        x = self.backbone.pos_drop(x)\n",
    "        x = self.backbone.blocks(x)\n",
    "        x = self.backbone.norm(x)\n",
    "        return self.backbone.head(x[:, 0])\n",
    "\n",
    "    def _compute_importance_scores(self, x_patches):\n",
    "        global_feat = x_patches.mean(dim=1, keepdim=True).expand(-1, x_patches.shape[1], -1)\n",
    "        selector_input = torch.cat([x_patches, global_feat], dim=-1)\n",
    "        return self.patch_selector(selector_input).squeeze(-1)\n",
    "\n",
    "    def forward_inference(self, x_images, k_ratio, policy='learned'):\n",
    "        x_patches = self._get_patch_embeddings(x_images)\n",
    "        B, N, D = x_patches.shape\n",
    "        k = int(N * k_ratio)\n",
    "        if k < 1:\n",
    "            k = 1\n",
    "\n",
    "        if policy == 'learned':\n",
    "            scores = self._compute_importance_scores(x_patches)\n",
    "            _, topk_idx = torch.topk(scores, k, dim=1)\n",
    "        elif policy == 'random':\n",
    "            topk_idx = torch.stack([torch.randperm(N)[:k] for _ in range(B)]).to(x_patches.device)\n",
    "        elif policy == 'central':\n",
    "            indices = self.central_indices[:k].to(x_patches.device)\n",
    "            topk_idx = indices.unsqueeze(0).expand(B, -1)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown policy for ASEL\")\n",
    "\n",
    "        topk_idx_expanded = topk_idx.unsqueeze(-1).expand(-1, -1, D)\n",
    "        x_kept = torch.gather(x_patches, 1, topk_idx_expanded)\n",
    "        return self._process_transformer(x_kept)\n",
    "\n",
    "class ASELBenchmark:\n",
    "    @staticmethod\n",
    "    def measure_metrics(model, device, k_ratio, policy='learned'):\n",
    "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "        batch_input = torch.randn(64, 3, 224, 224).to(device)\n",
    "\n",
    "        class Wrapper(nn.Module):\n",
    "            def __init__(self, m, k, p):\n",
    "                super().__init__()\n",
    "                self.m, self.k, self.p = m, k, p\n",
    "            def forward(self, x):\n",
    "                return self.m.forward_inference(x, self.k, self.p)\n",
    "\n",
    "        wrapped_model = Wrapper(model, k_ratio, policy).to(device)\n",
    "\n",
    "        if HAS_FVCORE:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                flops_counter = FlopCountAnalysis(wrapped_model, dummy_input)\n",
    "                flops_counter.unsupported_ops_warnings(False)\n",
    "                gflops = flops_counter.total() / 1e9\n",
    "        else:\n",
    "            gflops = 1.1 * k_ratio\n",
    "\n",
    "        model.eval()\n",
    "        start_event = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        end_event = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _ = model.forward_inference(batch_input, k_ratio, policy)\n",
    "            if torch.cuda.is_available():\n",
    "                start_event.record()\n",
    "                for _ in range(50):\n",
    "                    _ = model.forward_inference(batch_input, k_ratio, policy)\n",
    "                end_event.record()\n",
    "                torch.cuda.synchronize()\n",
    "                total_time_ms = start_event.elapsed_time(end_event)\n",
    "            else:\n",
    "                start = time.time()\n",
    "                for _ in range(50):\n",
    "                    _ = model.forward_inference(batch_input, k_ratio, policy)\n",
    "                total_time_ms = (time.time() - start) * 1000\n",
    "\n",
    "            latency_ms = total_time_ms / 50\n",
    "            throughput = (64 * 50) / (total_time_ms / 1000)\n",
    "\n",
    "        return gflops, latency_ms, throughput\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_accuracy(model, loader, device, k_ratio, policy='learned'):\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                logits = model.forward_inference(imgs, k_ratio=k_ratio, policy=policy)\n",
    "                correct += (logits.argmax(1) == labels).sum().item()\n",
    "                total += imgs.size(0)\n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "print(\"âœ“ ASEL model and benchmark helpers loaded (for comparison)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "136de964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.525263Z",
     "iopub.status.busy": "2025-12-26T13:55:25.525192Z",
     "iopub.status.idle": "2025-12-26T13:55:25.533674Z",
     "shell.execute_reply": "2025-12-26T13:55:25.533377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Comparative plotting utilities (ASEL vs AgentViT) ready\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPARATIVE PLOTS: ASEL VS AGENTVIT FROM SAVED MODELS\n",
    "# ==============================================================================\n",
    "ASEL_CHECKPOINTS = {\n",
    "    'aid': os.path.join(CONFIG['save_path'], 'aid_finetuned.pth'),\n",
    "    'eurosat': os.path.join(CONFIG['save_path'], 'eurosat_finetuned.pth'),\n",
    "    'rsscn7': os.path.join(CONFIG['save_path'], 'rsscn7_finetuned.pth'),\n",
    "}\n",
    "\n",
    "def _load_state_dict_flexible(obj):\n",
    "    \"\"\"Handle different checkpoint formats for robustness.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        for key in ['model_state', 'state_dict']:\n",
    "            if key in obj and isinstance(obj[key], dict):\n",
    "                return obj[key]\n",
    "        if all(isinstance(v, torch.Tensor) for v in obj.values()):\n",
    "            return obj\n",
    "    return obj\n",
    "\n",
    "def load_asel_model(ds_name, device):\n",
    "    ckpt_path = ASEL_CHECKPOINTS.get(ds_name)\n",
    "    if ckpt_path is None or not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"ASEL checkpoint not found for {ds_name}: {ckpt_path}\")\n",
    "\n",
    "    _, test_ds, n_cls = get_dataset(ds_name)\n",
    "    model = ASEL(num_classes=n_cls, pretrained=False).to(device)\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state_dict = _load_state_dict_flexible(ckpt)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "    return model, test_ds\n",
    "\n",
    "def load_agentvit_model(ds_name, device):\n",
    "    ckpt_path = os.path.join(CONFIG['save_path'], f'agentvit_{ds_name}.pth')\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"AgentViT checkpoint not found for {ds_name}: {ckpt_path}\")\n",
    "\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    cfg = ckpt.get('config', {})\n",
    "    image_size = cfg.get('resize_dim', CONFIG['resize_dim'])\n",
    "    patch_size = cfg.get('patch_size', 16)\n",
    "    embed_dim = cfg.get('embed_dim', 128)\n",
    "    depth = cfg.get('depth', 6)\n",
    "    heads = cfg.get('heads', 8)\n",
    "    mlp_dim = cfg.get('mlp_dim', 512)\n",
    "\n",
    "    _, test_ds, n_cls = get_dataset(ds_name)\n",
    "    model = SimpleAgentViT(\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        num_classes=n_cls,\n",
    "        dim=embed_dim,\n",
    "        depth=depth,\n",
    "        heads=heads,\n",
    "        mlp_dim=mlp_dim,\n",
    "    ).to(device)\n",
    "    state_dict = _load_state_dict_flexible(ckpt.get('model_state', ckpt))\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    model.eval()\n",
    "\n",
    "    num_patches_per_side = image_size // patch_size\n",
    "    num_patches = num_patches_per_side * num_patches_per_side\n",
    "    dqn_cfg = cfg if cfg else {}\n",
    "    dqn_agent = DQNAgent(\n",
    "        replay_batch_size=dqn_cfg.get('replay_batch_size', 8),\n",
    "        num_patches=num_patches,\n",
    "        buffer_capacity=dqn_cfg.get('buffer_capacity', 64),\n",
    "        gamma=dqn_cfg.get('gamma', 0.95),\n",
    "        tau=dqn_cfg.get('tau', 0.1),\n",
    "        update_frequency=dqn_cfg.get('update_frequency', 2),\n",
    "        learning_rate=dqn_cfg.get('dqn_lr', 0.01),\n",
    "        env=None,\n",
    "        device=device,\n",
    "        num_classes=n_cls,\n",
    "    )\n",
    "    if isinstance(ckpt, dict) and 'dqn_state' in ckpt:\n",
    "        dqn_agent.q_network.load_state_dict(ckpt['dqn_state'], strict=False)\n",
    "    return model, dqn_agent, test_ds\n",
    "\n",
    "def compare_asel_agentvit_on_dataset(ds_name):\n",
    "    device = CONFIG['device']\n",
    "    print(f\"\\n{'='*80}\\nCOMPARATIVE EVALUATION: {ds_name.upper()} (ASEL vs AgentViT)\\n{'='*80}\")\n",
    "\n",
    "    asel_model, asel_test_ds = load_asel_model(ds_name, device)\n",
    "    agentvit_model, dqn_agent, agent_test_ds = load_agentvit_model(ds_name, device)\n",
    "\n",
    "    # Use the same test set for both methods\n",
    "    _, test_ds, _ = get_dataset(ds_name)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'],\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    ratios = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    res = {\n",
    "        'ratios': ratios,\n",
    "        'asel_acc': [],\n",
    "        'agentvit_acc': [],\n",
    "        'asel_gflops': [],\n",
    "        'agentvit_gflops': [],\n",
    "        'asel_thr': [],\n",
    "        'agentvit_thr': [],\n",
    "        'asel_lat': [],\n",
    "        'agentvit_lat': [],\n",
    "    }\n",
    "\n",
    "    for r in ratios:\n",
    "        print(f\"  Evaluating keep ratio {r:.1f}...\")\n",
    "        acc_asel = ASELBenchmark.evaluate_accuracy(asel_model, test_loader, device, r, policy='learned')\n",
    "        gf_asel, lat_asel, thr_asel = ASELBenchmark.measure_metrics(asel_model, device, r, policy='learned')\n",
    "        acc_agent = AgentViTBenchmark.evaluate_accuracy_at_ratio(\n",
    "            agentvit_model, test_loader, device, r, use_learned_selection=True, dqn_agent=dqn_agent\n",
    ")\n",
    "        gf_agent, lat_agent, thr_agent = AgentViTBenchmark.measure_metrics(agentvit_model, device, r)\n",
    "\n",
    "        res['asel_acc'].append(acc_asel)\n",
    "        res['agentvit_acc'].append(acc_agent)\n",
    "        res['asel_gflops'].append(gf_asel)\n",
    "        res['agentvit_gflops'].append(gf_agent)\n",
    "        res['asel_thr'].append(thr_asel)\n",
    "        res['agentvit_thr'].append(thr_agent)\n",
    "        res['asel_lat'].append(lat_asel)\n",
    "        res['agentvit_lat'].append(lat_agent)\n",
    "\n",
    "    results_path = CONFIG['results_path']\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "    # 1. Accuracy vs Keep Ratio\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ratios, [x * 100 for x in res['asel_acc']], 'b-o', lw=2, label='ASEL (learned)')\n",
    "    plt.plot(ratios, [x * 100 for x in res['agentvit_acc']], 'r-s', lw=2, label='AgentViT (learned)')\n",
    "    plt.title(f'{ds_name.upper()}: Accuracy vs Keep Ratio (ASEL vs AgentViT)')\n",
    "    plt.xlabel('Keep Ratio')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_path, f'{ds_name}_asel_vs_agentvit_1_acc_keep.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Accuracy vs Throughput\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(res['asel_thr'], [x * 100 for x in res['asel_acc']], 'b-o', lw=2, label='ASEL')\n",
    "    plt.plot(res['agentvit_thr'], [x * 100 for x in res['agentvit_acc']], 'r-s', lw=2, label='AgentViT')\n",
    "    plt.title(f'{ds_name.upper()}: Accuracy vs Throughput (ASEL vs AgentViT)')\n",
    "    plt.xlabel('Throughput (img/s)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_path, f'{ds_name}_asel_vs_agentvit_2_acc_throughput.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3. GFLOPs vs Keep Ratio\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(ratios, res['asel_gflops'], 'b-o', lw=2, label='ASEL')\n",
    "    plt.plot(ratios, res['agentvit_gflops'], 'r-s', lw=2, label='AgentViT')\n",
    "    plt.title(f'{ds_name.upper()}: GFLOPs vs Keep Ratio (ASEL vs AgentViT)')\n",
    "    plt.xlabel('Keep Ratio')\n",
    "    plt.ylabel('GFLOPs')\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(results_path, f'{ds_name}_asel_vs_agentvit_3_gflops.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Latency at 50% keep ratio\n",
    "    mid_idx = 4  # ratio 0.5\n",
    "    asel_lat_50 = res['asel_lat'][mid_idx]\n",
    "    agent_lat_50 = res['agentvit_lat'][mid_idx]\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.bar(['ASEL (50%)', 'AgentViT (50%)'], [asel_lat_50, agent_lat_50], color=['blue', 'red'], width=0.5)\n",
    "    plt.title(f'{ds_name.upper()}: Batch Latency at 50% Keep (ASEL vs AgentViT)')\n",
    "    plt.ylabel('Time (ms)')\n",
    "    plt.text(0, asel_lat_50, f'{asel_lat_50:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.text(1, agent_lat_50, f'{agent_lat_50:.1f}ms', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.savefig(os.path.join(results_path, f'{ds_name}_asel_vs_agentvit_4_latency_50.png'))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"âœ“ Saved comparative ASEL vs AgentViT plots for {ds_name} to {results_path}\")\n",
    "    return res\n",
    "\n",
    "def run_asel_agentvit_comparisons():\n",
    "    for ds_name in ['aid', 'eurosat', 'rsscn7']:\n",
    "        try:\n",
    "            compare_asel_agentvit_on_dataset(ds_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error comparing {ds_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(\"âœ“ Comparative plotting utilities (ASEL vs AgentViT) ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f711cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T13:55:25.534573Z",
     "iopub.status.busy": "2025-12-26T13:55:25.534503Z",
     "iopub.status.idle": "2025-12-26T13:57:00.745652Z",
     "shell.execute_reply": "2025-12-26T13:57:00.745254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARATIVE EVALUATION: AID (ASEL vs AgentViT)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved comparative ASEL vs AgentViT plots for aid to ./benchmarks_results_agentvit\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE EVALUATION: EUROSAT (ASEL vs AgentViT)\n",
      "================================================================================\n",
      "  Evaluating keep ratio 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved comparative ASEL vs AgentViT plots for eurosat to ./benchmarks_results_agentvit\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE EVALUATION: RSSCN7 (ASEL vs AgentViT)\n",
      "================================================================================\n",
      "  Evaluating keep ratio 0.1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 0.9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Evaluating keep ratio 1.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "m.backbone.blocks.0.attn.attn_drop, m.backbone.blocks.1.attn.attn_drop, m.backbone.blocks.10.attn.attn_drop, m.backbone.blocks.11.attn.attn_drop, m.backbone.blocks.2.attn.attn_drop, m.backbone.blocks.3.attn.attn_drop, m.backbone.blocks.4.attn.attn_drop, m.backbone.blocks.5.attn.attn_drop, m.backbone.blocks.6.attn.attn_drop, m.backbone.blocks.7.attn.attn_drop, m.backbone.blocks.8.attn.attn_drop, m.backbone.blocks.9.attn.attn_drop, m.backbone.head_drop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved comparative ASEL vs AgentViT plots for rsscn7 to ./benchmarks_results_agentvit\n"
     ]
    }
   ],
   "source": [
    "run_asel_agentvit_comparisons()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
